{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c382763-f6e6-4f43-8dd7-0523cd64126d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVANCED LIGHTGBM MULTI-CLASS CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "[1/10] Loading data...\n",
      "Training data shape: (27617, 411)\n",
      "Test data shape: (13082, 411)\n",
      "\n",
      "[2/10] Advanced preprocessing...\n",
      "After cleaning: 27617 samples, 411 features\n",
      "\n",
      "Class distribution:\n",
      "  Class 1: 8874 (32.13%)\n",
      "  Class 2: 6127 (22.19%)\n",
      "  Class 3: 8483 (30.72%)\n",
      "  Class 4: 4133 (14.97%)\n",
      "\n",
      "[3/10] Multi-strategy imputation...\n",
      "Features with >50% missing: 0\n",
      "Features with 0-50% missing: 1\n",
      "Added 0 missingness indicators\n",
      "\n",
      "[4/10] Feature engineering...\n",
      "Created 10 statistical features\n",
      "\n",
      "[5/10] Feature selection and transformation...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADVANCED LIGHTGBM MULTI-CLASS CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load data\n",
    "# -----------------------------\n",
    "print(\"\\n[1/10] Loading data...\")\n",
    "X = pd.read_csv('trainingData.txt', header=None)\n",
    "y = pd.read_csv('trainingTruth.txt', header=None, names=['label']).squeeze()\n",
    "test_data = pd.read_csv('testData.txt', header=None)\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Advanced Data Preprocessing\n",
    "# -----------------------------\n",
    "print(\"\\n[2/10] Advanced preprocessing...\")\n",
    "\n",
    "# Replace empty strings with NaN and convert to numeric\n",
    "X = X.replace('', np.nan).apply(pd.to_numeric, errors='coerce')\n",
    "test_data = test_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Remove rows where y is null\n",
    "valid_mask = ~y.isna()\n",
    "X = X[valid_mask].reset_index(drop=True)\n",
    "y = y[valid_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"After cleaning: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "class_counts = y.value_counts().sort_index()\n",
    "for cls in class_counts.index:\n",
    "    print(f\"  Class {int(cls)}: {class_counts[cls]} ({100*class_counts[cls]/len(y):.2f}%)\")\n",
    "\n",
    "is_imbalanced = (class_counts.max() / class_counts.min()) > 1.5\n",
    "y = y - 1  # Zero-based\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Smart Imputation with Multiple Strategies\n",
    "# -----------------------------\n",
    "print(\"\\n[3/10] Multi-strategy imputation...\")\n",
    "\n",
    "# Analyze missingness pattern per feature\n",
    "missing_pct = X.isna().sum() / len(X) * 100\n",
    "high_missing = missing_pct[missing_pct > 50].index.tolist()\n",
    "low_missing = missing_pct[(missing_pct > 0) & (missing_pct <= 50)].index.tolist()\n",
    "\n",
    "print(f\"Features with >50% missing: {len(high_missing)}\")\n",
    "print(f\"Features with 0-50% missing: {len(low_missing)}\")\n",
    "\n",
    "# Create multiple imputed versions\n",
    "imputers = {\n",
    "    'median': SimpleImputer(strategy='median'),\n",
    "    'mean': SimpleImputer(strategy='mean'),\n",
    "}\n",
    "\n",
    "X_imputed_median = imputers['median'].fit_transform(X)\n",
    "X_imputed_mean = imputers['mean'].fit_transform(X)\n",
    "\n",
    "test_imputed_median = imputers['median'].transform(test_data)\n",
    "test_imputed_mean = imputers['mean'].transform(test_data)\n",
    "\n",
    "# Add missingness indicators as features (can be informative)\n",
    "missing_indicators = X.isna().astype(int).values\n",
    "test_missing_indicators = test_data.isna().astype(int).values\n",
    "\n",
    "# Select top missing indicators (those with >5% missingness)\n",
    "important_missing_cols = (missing_pct > 5).values\n",
    "missing_indicators = missing_indicators[:, important_missing_cols]\n",
    "test_missing_indicators = test_missing_indicators[:, important_missing_cols]\n",
    "\n",
    "print(f\"Added {missing_indicators.shape[1]} missingness indicators\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Advanced Feature Engineering\n",
    "# -----------------------------\n",
    "print(\"\\n[4/10] Feature engineering...\")\n",
    "\n",
    "X_base = X_imputed_median.copy()\n",
    "test_base = test_imputed_median.copy()\n",
    "\n",
    "# Statistical features per row\n",
    "row_features = []\n",
    "test_row_features = []\n",
    "\n",
    "for data in [X_base, test_base]:\n",
    "    features = []\n",
    "    features.append(np.mean(data, axis=1).reshape(-1, 1))  # Mean\n",
    "    features.append(np.std(data, axis=1).reshape(-1, 1))   # Std\n",
    "    features.append(np.median(data, axis=1).reshape(-1, 1))  # Median\n",
    "    features.append(np.min(data, axis=1).reshape(-1, 1))   # Min\n",
    "    features.append(np.max(data, axis=1).reshape(-1, 1))   # Max\n",
    "    features.append((np.max(data, axis=1) - np.min(data, axis=1)).reshape(-1, 1))  # Range\n",
    "    features.append(stats.skew(data, axis=1).reshape(-1, 1))  # Skewness\n",
    "    features.append(stats.kurtosis(data, axis=1).reshape(-1, 1))  # Kurtosis\n",
    "    features.append(np.percentile(data, 25, axis=1).reshape(-1, 1))  # Q1\n",
    "    features.append(np.percentile(data, 75, axis=1).reshape(-1, 1))  # Q3\n",
    "    \n",
    "    if data is X_base:\n",
    "        row_features = np.hstack(features)\n",
    "    else:\n",
    "        test_row_features = np.hstack(features)\n",
    "\n",
    "print(f\"Created {row_features.shape[1]} statistical features\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Feature Selection and Transformation\n",
    "# -----------------------------\n",
    "print(\"\\n[5/10] Feature selection and transformation...\")\n",
    "\n",
    "# Calculate mutual information\n",
    "mi_scores = mutual_info_classif(X_base, y, random_state=42, n_neighbors=5)\n",
    "mi_threshold = np.percentile(mi_scores, 25)  # Keep top 75%\n",
    "selected_features = mi_scores > mi_threshold\n",
    "\n",
    "print(f\"Selected {selected_features.sum()} features based on mutual information\")\n",
    "\n",
    "X_selected = X_base[:, selected_features]\n",
    "test_selected = test_base[:, selected_features]\n",
    "\n",
    "# Apply QuantileTransformer for better distribution\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "X_quantile = quantile_transformer.fit_transform(X_selected)\n",
    "test_quantile = quantile_transformer.transform(test_selected)\n",
    "\n",
    "# Create PCA features for additional representation\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "X_pca = pca.fit_transform(X_quantile)\n",
    "test_pca = pca.transform(test_quantile)\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Combine all feature sets\n",
    "X_final = np.hstack([\n",
    "    X_selected,           # Selected original features\n",
    "    X_quantile,           # Quantile-transformed features\n",
    "    X_pca,                # PCA features\n",
    "    row_features,         # Statistical features\n",
    "    missing_indicators    # Missingness indicators\n",
    "])\n",
    "\n",
    "test_final = np.hstack([\n",
    "    test_selected,\n",
    "    test_quantile,\n",
    "    test_pca,\n",
    "    test_row_features,\n",
    "    test_missing_indicators\n",
    "])\n",
    "\n",
    "print(f\"Final feature count: {X_final.shape[1]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Optimized Hyperparameters\n",
    "# -----------------------------\n",
    "print(\"\\n[6/10] Configuring optimized hyperparameters...\")\n",
    "\n",
    "param_configs = [\n",
    "    {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'metric': 'multi_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': 10,\n",
    "        'min_data_in_leaf': 15,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'lambda_l1': 0.5,\n",
    "        'lambda_l2': 0.5,\n",
    "        'min_gain_to_split': 0.001,\n",
    "        'path_smooth': 1.0,\n",
    "        'verbose': -1,\n",
    "        'is_unbalance': is_imbalanced,\n",
    "        'seed': 42\n",
    "    },\n",
    "    {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'metric': 'multi_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.008,\n",
    "        'num_leaves': 95,\n",
    "        'max_depth': 12,\n",
    "        'min_data_in_leaf': 12,\n",
    "        'feature_fraction': 0.75,\n",
    "        'bagging_fraction': 0.75,\n",
    "        'bagging_freq': 4,\n",
    "        'lambda_l1': 0.3,\n",
    "        'lambda_l2': 0.7,\n",
    "        'min_gain_to_split': 0.001,\n",
    "        'path_smooth': 0.5,\n",
    "        'verbose': -1,\n",
    "        'is_unbalance': is_imbalanced,\n",
    "        'seed': 123\n",
    "    },\n",
    "    {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'metric': 'multi_logloss',\n",
    "        'boosting_type': 'dart',  # Different boosting type\n",
    "        'learning_rate': 0.015,\n",
    "        'num_leaves': 80,\n",
    "        'max_depth': 9,\n",
    "        'min_data_in_leaf': 18,\n",
    "        'feature_fraction': 0.85,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 3,\n",
    "        'lambda_l1': 0.2,\n",
    "        'lambda_l2': 0.5,\n",
    "        'drop_rate': 0.1,\n",
    "        'skip_drop': 0.5,\n",
    "        'verbose': -1,\n",
    "        'is_unbalance': is_imbalanced,\n",
    "        'seed': 456\n",
    "    },\n",
    "    {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'metric': 'multi_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.012,\n",
    "        'num_leaves': 110,\n",
    "        'max_depth': 11,\n",
    "        'min_data_in_leaf': 10,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 2,\n",
    "        'lambda_l1': 0.1,\n",
    "        'lambda_l2': 0.3,\n",
    "        'min_gain_to_split': 0.0005,\n",
    "        'path_smooth': 1.5,\n",
    "        'verbose': -1,\n",
    "        'is_unbalance': is_imbalanced,\n",
    "        'seed': 789\n",
    "    },\n",
    "    {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'metric': 'multi_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.02,\n",
    "        'num_leaves': 63,\n",
    "        'max_depth': 8,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'feature_fraction': 0.7,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 6,\n",
    "        'lambda_l1': 0.8,\n",
    "        'lambda_l2': 1.0,\n",
    "        'min_gain_to_split': 0.002,\n",
    "        'path_smooth': 0.8,\n",
    "        'verbose': -1,\n",
    "        'is_unbalance': is_imbalanced,\n",
    "        'seed': 2024\n",
    "    }\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Cross-Validation with Stratified Folds\n",
    "# -----------------------------\n",
    "print(\"\\n[7/10] Cross-validation training...\")\n",
    "\n",
    "n_folds = 10  # More folds for better CV estimate\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "all_config_scores = []\n",
    "\n",
    "for config_idx, params in enumerate(param_configs):\n",
    "    print(f\"\\n--- Configuration {config_idx + 1}/{len(param_configs)} ---\")\n",
    "    \n",
    "    fold_scores = []\n",
    "    fold_models = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_final, y)):\n",
    "        X_train, X_val = X_final[train_idx], X_final[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_dataset = lgb.Dataset(X_val, label=y_val, reference=train_dataset)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_dataset,\n",
    "            num_boost_round=3000,\n",
    "            valid_sets=[valid_dataset],\n",
    "            callbacks=[\n",
    "                early_stopping(stopping_rounds=150),\n",
    "                log_evaluation(period=0)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        y_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "        accuracy = accuracy_score(y_val, y_val_pred_labels)\n",
    "        \n",
    "        fold_scores.append(accuracy)\n",
    "        fold_models.append(model)\n",
    "    \n",
    "    avg_score = np.mean(fold_scores)\n",
    "    std_score = np.std(fold_scores)\n",
    "    print(f\"  → CV Score: {avg_score:.4f} ± {std_score:.4f}\")\n",
    "    \n",
    "    all_config_scores.append((avg_score, std_score, config_idx, fold_models))\n",
    "\n",
    "# Select best configuration\n",
    "best_score, best_std, best_config_idx, best_fold_models = max(all_config_scores, key=lambda x: x[0])\n",
    "best_params = param_configs[best_config_idx]\n",
    "\n",
    "print(f\"\\n✓ Best configuration: Config {best_config_idx + 1}\")\n",
    "print(f\"  CV Score: {best_score:.4f} ± {best_std:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Validation Metrics\n",
    "# -----------------------------\n",
    "print(\"\\n[8/10] Computing validation metrics...\")\n",
    "\n",
    "all_val_preds = []\n",
    "all_val_true = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_final, y)):\n",
    "    X_val = X_final[val_idx]\n",
    "    y_val = y.iloc[val_idx]\n",
    "    \n",
    "    model = best_fold_models[fold]\n",
    "    y_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    all_val_preds.append(y_val_pred)\n",
    "    all_val_true.extend(y_val.values)\n",
    "\n",
    "all_val_preds = np.vstack(all_val_preds)\n",
    "all_val_true = np.array(all_val_true)\n",
    "val_pred_labels = np.argmax(all_val_preds, axis=1)\n",
    "\n",
    "print(f\"\\nOverall Validation Accuracy: {accuracy_score(all_val_true, val_pred_labels):.4f}\")\n",
    "\n",
    "print(\"\\nClass-wise AUC scores:\")\n",
    "for i in range(4):\n",
    "    y_true_bin = (all_val_true == i).astype(int)\n",
    "    auc = roc_auc_score(y_true_bin, all_val_preds[:, i])\n",
    "    print(f\"  Class {i+1} AUC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_val_true, val_pred_labels, \n",
    "                          target_names=[f'Class {i+1}' for i in range(4)],\n",
    "                          digits=4))\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Diverse Ensemble Prediction\n",
    "# -----------------------------\n",
    "print(\"\\n[9/10] Generating diverse ensemble predictions...\")\n",
    "\n",
    "ensemble_test_preds = []\n",
    "\n",
    "# Strategy 1: Use all CV fold models\n",
    "for fold_idx, model in enumerate(best_fold_models):\n",
    "    test_pred = model.predict(test_final, num_iteration=model.best_iteration)\n",
    "    ensemble_test_preds.append(test_pred)\n",
    "    print(f\"  CV fold model {fold_idx + 1}/{len(best_fold_models)}\")\n",
    "\n",
    "# Strategy 2: Train additional models with different seeds\n",
    "additional_seeds = [2025, 3141, 9876, 5555, 7777]\n",
    "avg_best_iter = int(np.mean([m.best_iteration for m in best_fold_models]))\n",
    "\n",
    "for seed in additional_seeds:\n",
    "    params_with_seed = best_params.copy()\n",
    "    params_with_seed['seed'] = seed\n",
    "    \n",
    "    full_train = lgb.Dataset(X_final, label=y)\n",
    "    model = lgb.train(params_with_seed, full_train, num_boost_round=avg_best_iter)\n",
    "    \n",
    "    test_pred = model.predict(test_final)\n",
    "    ensemble_test_preds.append(test_pred)\n",
    "\n",
    "print(f\"  Total ensemble models: {len(ensemble_test_preds)}\")\n",
    "\n",
    "# Weighted ensemble (give more weight to CV models)\n",
    "weights = [1.2] * len(best_fold_models) + [1.0] * len(additional_seeds)\n",
    "weights = np.array(weights) / sum(weights)\n",
    "\n",
    "test_pred_final = np.average(ensemble_test_preds, axis=0, weights=weights)\n",
    "test_labels = np.argmax(test_pred_final, axis=1) + 1\n",
    "\n",
    "# Prediction statistics\n",
    "prediction_confidence = np.max(test_pred_final, axis=1)\n",
    "print(f\"\\nPrediction confidence: {prediction_confidence.mean():.4f} ± {prediction_confidence.std():.4f}\")\n",
    "\n",
    "print(\"\\nPredicted class distribution:\")\n",
    "pred_counts = pd.Series(test_labels).value_counts().sort_index()\n",
    "for cls in pred_counts.index:\n",
    "    print(f\"  Class {int(cls)}: {pred_counts[cls]} ({100*pred_counts[cls]/len(test_labels):.2f}%)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Save Results\n",
    "# -----------------------------\n",
    "print(\"\\n[10/10] Saving results...\")\n",
    "\n",
    "output = np.column_stack([test_pred_final, test_labels])\n",
    "np.savetxt('testLabel_lightgbm_improved.txt', output, \n",
    "           fmt='%.6f\\t%.6f\\t%.6f\\t%.6f\\t%d', \n",
    "           delimiter='\\t')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nResults saved to 'testLabel_lightgbm_improved.txt'\")\n",
    "print(f\"Expected accuracy: {best_score:.4f} ± {best_std:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951be4a-a7f1-4fab-8153-e567fd0d6498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a703c-422e-4974-995d-95cf27779165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
