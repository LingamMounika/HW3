{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34230d8-47e6-4a79-b299-47c7628e318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RANDOM FOREST MULTI-CLASS CLASSIFICATION (NOISE-ROBUST)\n",
      "================================================================================\n",
      "\n",
      "[1/8] Loading data...\n",
      "Training data shape: (27617, 411)\n",
      "Test data shape: (13082, 411)\n",
      "\n",
      "[2/8] Preprocessing data...\n",
      "After cleaning: 27617 samples, 411 features\n",
      "Features with missing values: 1/411\n",
      "  Max missing %: 0.50%\n",
      "  Mean missing %: 0.50%\n",
      "\n",
      "Class distribution:\n",
      "  Class 1: 8874 samples (32.13%)\n",
      "  Class 2: 6127 samples (22.19%)\n",
      "  Class 3: 8483 samples (30.72%)\n",
      "  Class 4: 4133 samples (14.97%)\n",
      "⚠️  Dataset appears imbalanced - will use balanced class weights\n",
      "\n",
      "[3/8] Applying median imputation...\n",
      "Feature count: 411\n",
      "\n",
      "[4/8] Configuring Random Forest parameters (noise-robust)...\n",
      "\n",
      "[5/8] Training with 5-fold cross-validation...\n",
      "\n",
      "--- Configuration 1/4 ---\n",
      "n_estimators: 500, max_depth: 20, min_samples_split: 20, max_features: sqrt\n",
      "  Fold 1: Accuracy = 0.8240\n",
      "  Fold 2: Accuracy = 0.8143\n",
      "  Fold 3: Accuracy = 0.8083\n",
      "  Fold 4: Accuracy = 0.8099\n",
      "  Fold 5: Accuracy = 0.8102\n",
      "  → CV Score: 0.8133 ± 0.0057\n",
      "\n",
      "--- Configuration 2/4 ---\n",
      "n_estimators: 500, max_depth: 25, min_samples_split: 15, max_features: sqrt\n",
      "  Fold 1: Accuracy = 0.8242\n",
      "  Fold 2: Accuracy = 0.8094\n",
      "  Fold 3: Accuracy = 0.8083\n",
      "  Fold 4: Accuracy = 0.8074\n",
      "  Fold 5: Accuracy = 0.8133\n",
      "  → CV Score: 0.8125 ± 0.0062\n",
      "\n",
      "--- Configuration 3/4 ---\n",
      "n_estimators: 600, max_depth: 18, min_samples_split: 25, max_features: sqrt\n",
      "  Fold 1: Accuracy = 0.8268\n",
      "  Fold 2: Accuracy = 0.8134\n",
      "  Fold 3: Accuracy = 0.8135\n",
      "  Fold 4: Accuracy = 0.8122\n",
      "  Fold 5: Accuracy = 0.8128\n",
      "  → CV Score: 0.8157 ± 0.0055\n",
      "\n",
      "--- Configuration 4/4 ---\n",
      "n_estimators: 400, max_depth: 22, min_samples_split: 18, max_features: log2\n",
      "  Fold 1: Accuracy = 0.8085\n",
      "  Fold 2: Accuracy = 0.7922\n",
      "  Fold 3: Accuracy = 0.7921\n",
      "  Fold 4: Accuracy = 0.7985\n",
      "  Fold 5: Accuracy = 0.7994\n",
      "  → CV Score: 0.7981 ± 0.0060\n",
      "\n",
      "✓ Best configuration: Config 3\n",
      "  CV Score: 0.8157 ± 0.0055\n",
      "\n",
      "[6/8] Detailed validation metrics...\n",
      "\n",
      "Overall Validation Accuracy: 0.8157\n",
      "\n",
      "Class-wise AUC scores:\n",
      "  Class 1 AUC: 0.9676\n",
      "  Class 2 AUC: 0.9586\n",
      "  Class 3 AUC: 0.9600\n",
      "  Class 4 AUC: 0.9200\n",
      "  Macro-average AUC: 0.9516\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 1     0.7926    0.9547    0.8661      8874\n",
      "     Class 2     0.7481    0.9120    0.8219      6127\n",
      "     Class 3     0.8938    0.7937    0.8408      8483\n",
      "     Class 4     0.9013    0.4198    0.5728      4133\n",
      "\n",
      "    accuracy                         0.8157     27617\n",
      "   macro avg     0.8339    0.7701    0.7754     27617\n",
      "weighted avg     0.8301    0.8157    0.8046     27617\n",
      "\n",
      "\n",
      "[7/8] Analyzing feature importance...\n",
      "\n",
      "Top 20 most important features:\n",
      "  Feature 85: 0.025401\n",
      "  Feature 86: 0.020564\n",
      "  Feature 84: 0.018000\n",
      "  Feature 356: 0.017471\n",
      "  Feature 357: 0.016606\n",
      "  Feature 336: 0.015717\n",
      "  Feature 205: 0.014288\n",
      "  Feature 124: 0.013853\n",
      "  Feature 337: 0.013671\n",
      "  Feature 225: 0.012614\n",
      "  Feature 185: 0.012310\n",
      "  Feature 165: 0.011677\n",
      "  Feature 105: 0.011183\n",
      "  Feature 346: 0.010676\n",
      "  Feature 88: 0.010561\n",
      "  Feature 354: 0.010187\n",
      "  Feature 347: 0.010165\n",
      "  Feature 144: 0.010060\n",
      "  Feature 94: 0.009725\n",
      "  Feature 87: 0.009534\n",
      "\n",
      "[8/8] Generating ensemble predictions on test set...\n",
      "  Training ensemble model 1/8 (seed=42)...\n",
      "  Training ensemble model 2/8 (seed=123)...\n",
      "  Training ensemble model 3/8 (seed=456)...\n",
      "  Training ensemble model 4/8 (seed=789)...\n",
      "  Training ensemble model 5/8 (seed=2024)...\n",
      "  Training ensemble model 6/8 (seed=2025)...\n",
      "  Training ensemble model 7/8 (seed=3141)...\n",
      "  Training ensemble model 8/8 (seed=9876)...\n",
      "\n",
      "Test prediction confidence statistics:\n",
      "  Mean: 0.4816\n",
      "  Median: 0.4515\n",
      "  Min: 0.2516\n",
      "  Max: 0.9189\n",
      "\n",
      "Test ensemble agreement statistics:\n",
      "  Mean agreement: 0.9690\n",
      "  High agreement (>0.8): 12157 samples\n",
      "  Low agreement (<0.5): 57 samples\n",
      "\n",
      "Test predicted class distribution:\n",
      "  Class 1: 5022 samples (38.39%)\n",
      "  Class 2: 3511 samples (26.84%)\n",
      "  Class 3: 3541 samples (27.07%)\n",
      "  Class 4: 1008 samples (7.71%)\n",
      "\n",
      "Class distribution comparison (Train → Test):\n",
      "  Class 1: 32.1% → 38.4% (Δ +6.3%)\n",
      "  Class 2: 22.2% → 26.8% (Δ +4.7%)\n",
      "  Class 3: 30.7% → 27.1% (Δ -3.6%)\n",
      "  Class 4: 15.0% → 7.7% (Δ -7.3%)\n",
      "\n",
      "✓ Test predictions saved to 'testLabel_rf.txt'\n",
      "✓ Confidence metrics saved to 'testLabel_rf_confidence.txt'\n",
      "\n",
      "================================================================================\n",
      "[9/9] Generating predictions for BLIND dataset...\n",
      "================================================================================\n",
      "\n",
      "⚠️  'blindData.txt' not found - skipping blind data prediction\n",
      "\n",
      "================================================================================\n",
      "✓ ALL PREDICTIONS COMPLETED SUCCESSFULLY\n",
      "================================================================================\n",
      "\n",
      "Files generated:\n",
      "  1. testLabel_rf.txt (13082 predictions)\n",
      "  2. testLabel_rf_confidence.txt (test confidence scores)\n",
      "\n",
      "Model Performance Summary:\n",
      "  - Algorithm: Random Forest (Noise-Robust)\n",
      "  - Ensemble size: 8 models\n",
      "  - Expected validation accuracy: 0.8157 ± 0.0055\n",
      "  - Expected macro-average AUC: 0.9516\n",
      "  - Best configuration: Config 3\n",
      "  - Noise resistance: bagging, depth limits, min_samples constraints\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RANDOM FOREST MULTI-CLASS CLASSIFICATION (NOISE-ROBUST)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load data\n",
    "# -----------------------------\n",
    "print(\"\\n[1/8] Loading data...\")\n",
    "X = pd.read_csv('trainingData.txt', header=None)\n",
    "y = pd.read_csv('trainingTruth.txt', header=None, names=['label']).squeeze()\n",
    "test_data = pd.read_csv('testData.txt', header=None)\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Preprocessing\n",
    "# -----------------------------\n",
    "print(\"\\n[2/8] Preprocessing data...\")\n",
    "\n",
    "# Replace empty strings with NaN and convert to numeric\n",
    "X = X.replace('', np.nan).apply(pd.to_numeric, errors='coerce')\n",
    "test_data = test_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Remove rows where y is null\n",
    "valid_mask = ~y.isna()\n",
    "X = X[valid_mask].reset_index(drop=True)\n",
    "y = y[valid_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"After cleaning: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "\n",
    "# Analyze missing data\n",
    "missing_percentage = (X.isna().sum() / len(X)) * 100\n",
    "features_with_missing = (missing_percentage > 0).sum()\n",
    "print(f\"Features with missing values: {features_with_missing}/{X.shape[1]}\")\n",
    "if features_with_missing > 0:\n",
    "    print(f\"  Max missing %: {missing_percentage.max():.2f}%\")\n",
    "    print(f\"  Mean missing %: {missing_percentage[missing_percentage > 0].mean():.2f}%\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "class_counts = y.value_counts().sort_index()\n",
    "for cls in class_counts.index:\n",
    "    print(f\"  Class {int(cls)}: {class_counts[cls]} samples ({100*class_counts[cls]/len(y):.2f}%)\")\n",
    "\n",
    "# Check for class imbalance\n",
    "is_imbalanced = (class_counts.max() / class_counts.min()) > 1.5\n",
    "if is_imbalanced:\n",
    "    print(\"⚠️  Dataset appears imbalanced - will use balanced class weights\")\n",
    "    class_weight = 'balanced'\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Imputation\n",
    "# -----------------------------\n",
    "print(\"\\n[3/8] Applying median imputation...\")\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "test_imputed = imputer.transform(test_data)\n",
    "\n",
    "print(f\"Feature count: {X_imputed.shape[1]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Random Forest Hyperparameter Configurations\n",
    "# -----------------------------\n",
    "print(\"\\n[4/8] Configuring Random Forest parameters (noise-robust)...\")\n",
    "\n",
    "# Random Forest parameters optimized for noisy data:\n",
    "# - n_estimators: More trees = better averaging and noise reduction\n",
    "# - max_depth: Limited depth prevents overfitting to noise\n",
    "# - min_samples_split: Higher values prevent learning from noisy samples\n",
    "# - min_samples_leaf: Ensures leaves have enough samples\n",
    "# - max_features: Controls randomness and prevents correlation between trees\n",
    "# - bootstrap: True for bagging (reduces variance)\n",
    "\n",
    "rf_configs = [\n",
    "    {\n",
    "        'n_estimators': 500,\n",
    "        'max_depth': 20,\n",
    "        'min_samples_split': 20,\n",
    "        'min_samples_leaf': 10,\n",
    "        'max_features': 'sqrt',\n",
    "        'bootstrap': True,\n",
    "        'class_weight': class_weight,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': 500,\n",
    "        'max_depth': 25,\n",
    "        'min_samples_split': 15,\n",
    "        'min_samples_leaf': 8,\n",
    "        'max_features': 'sqrt',\n",
    "        'bootstrap': True,\n",
    "        'class_weight': class_weight,\n",
    "        'random_state': 123,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': 600,\n",
    "        'max_depth': 18,\n",
    "        'min_samples_split': 25,\n",
    "        'min_samples_leaf': 12,\n",
    "        'max_features': 'sqrt',\n",
    "        'bootstrap': True,\n",
    "        'class_weight': class_weight,\n",
    "        'random_state': 456,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': 400,\n",
    "        'max_depth': 22,\n",
    "        'min_samples_split': 18,\n",
    "        'min_samples_leaf': 9,\n",
    "        'max_features': 'log2',\n",
    "        'bootstrap': True,\n",
    "        'class_weight': class_weight,\n",
    "        'random_state': 789,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': 0\n",
    "    }\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Cross-Validation Training\n",
    "# -----------------------------\n",
    "print(\"\\n[5/8] Training with 5-fold cross-validation...\")\n",
    "\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "all_config_scores = []\n",
    "\n",
    "for config_idx, params in enumerate(rf_configs):\n",
    "    print(f\"\\n--- Configuration {config_idx + 1}/{len(rf_configs)} ---\")\n",
    "    print(f\"n_estimators: {params['n_estimators']}, max_depth: {params['max_depth']}, \"\n",
    "          f\"min_samples_split: {params['min_samples_split']}, max_features: {params['max_features']}\")\n",
    "    \n",
    "    fold_scores = []\n",
    "    fold_models = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_imputed, y)):\n",
    "        X_train, X_val = X_imputed[train_idx], X_imputed[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Validate\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        \n",
    "        fold_scores.append(accuracy)\n",
    "        fold_models.append(model)\n",
    "        \n",
    "        print(f\"  Fold {fold + 1}: Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    # Calculate average CV score\n",
    "    avg_score = np.mean(fold_scores)\n",
    "    std_score = np.std(fold_scores)\n",
    "    print(f\"  → CV Score: {avg_score:.4f} ± {std_score:.4f}\")\n",
    "    \n",
    "    all_config_scores.append((avg_score, std_score, config_idx, fold_models, params))\n",
    "\n",
    "# Select best configuration\n",
    "best_score, best_std, best_config_idx, best_fold_models, best_params = max(all_config_scores, key=lambda x: x[0])\n",
    "\n",
    "print(f\"\\n✓ Best configuration: Config {best_config_idx + 1}\")\n",
    "print(f\"  CV Score: {best_score:.4f} ± {best_std:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Validation Metrics\n",
    "# -----------------------------\n",
    "print(\"\\n[6/8] Detailed validation metrics...\")\n",
    "\n",
    "all_val_preds = []\n",
    "all_val_probs = []\n",
    "all_val_true = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_imputed, y)):\n",
    "    X_val = X_imputed[val_idx]\n",
    "    y_val = y.iloc[val_idx]\n",
    "    \n",
    "    model = best_fold_models[fold]\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_probs = model.predict_proba(X_val)\n",
    "    \n",
    "    all_val_preds.extend(y_val_pred)\n",
    "    all_val_probs.append(y_val_probs)\n",
    "    all_val_true.extend(y_val.values)\n",
    "\n",
    "all_val_probs = np.vstack(all_val_probs)\n",
    "all_val_preds = np.array(all_val_preds)\n",
    "all_val_true = np.array(all_val_true)\n",
    "\n",
    "accuracy = accuracy_score(all_val_true, all_val_preds)\n",
    "\n",
    "print(f\"\\nOverall Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClass-wise AUC scores:\")\n",
    "auc_scores = []\n",
    "for i in range(4):\n",
    "    y_true_bin = (all_val_true == (i+1)).astype(int)\n",
    "    auc = roc_auc_score(y_true_bin, all_val_probs[:, i])\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"  Class {i+1} AUC: {auc:.4f}\")\n",
    "\n",
    "macro_auc = np.mean(auc_scores)\n",
    "print(f\"  Macro-average AUC: {macro_auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_val_true, all_val_preds, \n",
    "                          target_names=[f'Class {i+1}' for i in range(4)],\n",
    "                          digits=4))\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Feature Importance\n",
    "# -----------------------------\n",
    "print(\"\\n[7/8] Analyzing feature importance...\")\n",
    "\n",
    "# Get feature importance from first model (MDI - Mean Decrease in Impurity)\n",
    "importance = best_fold_models[0].feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature_idx': range(len(importance)),\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "for idx, row in importance_df.head(20).iterrows():\n",
    "    print(f\"  Feature {int(row['feature_idx'])}: {row['importance']:.6f}\")\n",
    "\n",
    "zero_importance = (importance == 0).sum()\n",
    "if zero_importance > 0:\n",
    "    print(f\"\\n⚠️  {zero_importance} features have zero importance (likely noise)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Ensemble Prediction on Test Set\n",
    "# -----------------------------\n",
    "print(\"\\n[8/8] Generating ensemble predictions on test set...\")\n",
    "\n",
    "ensemble_test_preds = []\n",
    "seeds = [42, 123, 456, 789, 2024, 2025, 3141, 9876]\n",
    "\n",
    "for seed_idx, seed in enumerate(seeds):\n",
    "    print(f\"  Training ensemble model {seed_idx + 1}/{len(seeds)} (seed={seed})...\")\n",
    "    \n",
    "    params_with_seed = best_params.copy()\n",
    "    params_with_seed['random_state'] = seed\n",
    "    \n",
    "    model = RandomForestClassifier(**params_with_seed)\n",
    "    model.fit(X_imputed, y)\n",
    "    \n",
    "    test_probs = model.predict_proba(test_imputed)\n",
    "    ensemble_test_preds.append(test_probs)\n",
    "\n",
    "# Average ensemble predictions\n",
    "test_pred_final = np.mean(ensemble_test_preds, axis=0)\n",
    "test_labels = np.argmax(test_pred_final, axis=1) + 1\n",
    "\n",
    "# Calculate confidence metrics\n",
    "prediction_confidence = np.max(test_pred_final, axis=1)\n",
    "ensemble_agreement = np.array([\n",
    "    np.mean([np.argmax(pred[i]) == np.argmax(test_pred_final[i]) \n",
    "             for pred in ensemble_test_preds])\n",
    "    for i in range(len(test_pred_final))\n",
    "])\n",
    "\n",
    "print(f\"\\nTest prediction confidence statistics:\")\n",
    "print(f\"  Mean: {prediction_confidence.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(prediction_confidence):.4f}\")\n",
    "print(f\"  Min: {prediction_confidence.min():.4f}\")\n",
    "print(f\"  Max: {prediction_confidence.max():.4f}\")\n",
    "\n",
    "print(f\"\\nTest ensemble agreement statistics:\")\n",
    "print(f\"  Mean agreement: {ensemble_agreement.mean():.4f}\")\n",
    "print(f\"  High agreement (>0.8): {(ensemble_agreement > 0.8).sum()} samples\")\n",
    "print(f\"  Low agreement (<0.5): {(ensemble_agreement < 0.5).sum()} samples\")\n",
    "\n",
    "# Show class distribution\n",
    "print(\"\\nTest predicted class distribution:\")\n",
    "pred_counts = pd.Series(test_labels).value_counts().sort_index()\n",
    "for cls in pred_counts.index:\n",
    "    print(f\"  Class {int(cls)}: {pred_counts[cls]} samples ({100*pred_counts[cls]/len(test_labels):.2f}%)\")\n",
    "\n",
    "# Compare with training\n",
    "print(\"\\nClass distribution comparison (Train → Test):\")\n",
    "for cls in sorted(class_counts.index):\n",
    "    train_pct = 100 * class_counts[cls] / len(y)\n",
    "    test_pct = 100 * pred_counts.get(cls, 0) / len(test_labels)\n",
    "    diff = test_pct - train_pct\n",
    "    print(f\"  Class {int(cls)}: {train_pct:.1f}% → {test_pct:.1f}% (Δ {diff:+.1f}%)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Save Test Results\n",
    "# -----------------------------\n",
    "output = np.column_stack([test_pred_final, test_labels])\n",
    "np.savetxt('testLabel_rf.txt', output, \n",
    "           fmt='%.6f\\t%.6f\\t%.6f\\t%.6f\\t%d', \n",
    "           delimiter='\\t')\n",
    "\n",
    "np.savetxt('testLabel_rf_confidence.txt', \n",
    "           np.column_stack([test_labels, prediction_confidence, ensemble_agreement]),\n",
    "           fmt='%d\\t%.6f\\t%.6f',\n",
    "           header='predicted_label\\tconfidence\\tensemble_agreement',\n",
    "           comments='')\n",
    "\n",
    "print(f\"\\n✓ Test predictions saved to 'testLabel_rf.txt'\")\n",
    "print(f\"✓ Confidence metrics saved to 'testLabel_rf_confidence.txt'\")\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Blind Data Prediction\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[9/9] Generating predictions for BLIND dataset...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    blind_data = pd.read_csv('blindData.txt', header=None)\n",
    "    print(f\"\\nBlind data shape: {blind_data.shape}\")\n",
    "    \n",
    "    blind_data = blind_data.apply(pd.to_numeric, errors='coerce')\n",
    "    blind_imputed = imputer.transform(blind_data)\n",
    "    \n",
    "    print(f\"Preprocessed blind data shape: {blind_imputed.shape}\")\n",
    "    print(f\"\\nGenerating ensemble predictions with {len(seeds)} models...\")\n",
    "    \n",
    "    ensemble_blind_preds = []\n",
    "    \n",
    "    for seed_idx, seed in enumerate(seeds):\n",
    "        print(f\"  Model {seed_idx + 1}/{len(seeds)}...\", end='\\r')\n",
    "        \n",
    "        params_with_seed = best_params.copy()\n",
    "        params_with_seed['random_state'] = seed\n",
    "        \n",
    "        model = RandomForestClassifier(**params_with_seed)\n",
    "        model.fit(X_imputed, y)\n",
    "        \n",
    "        blind_probs = model.predict_proba(blind_imputed)\n",
    "        ensemble_blind_preds.append(blind_probs)\n",
    "    \n",
    "    print(f\"  Completed all {len(seeds)} models    \")\n",
    "    \n",
    "    # Average predictions\n",
    "    blind_pred_final = np.mean(ensemble_blind_preds, axis=0)\n",
    "    blind_labels = np.argmax(blind_pred_final, axis=1) + 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    blind_confidence = np.max(blind_pred_final, axis=1)\n",
    "    blind_agreement = np.array([\n",
    "        np.mean([np.argmax(pred[i]) == np.argmax(blind_pred_final[i]) \n",
    "                 for pred in ensemble_blind_preds])\n",
    "        for i in range(len(blind_pred_final))\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nBlind prediction confidence statistics:\")\n",
    "    print(f\"  Mean: {blind_confidence.mean():.4f}\")\n",
    "    print(f\"  Median: {np.median(blind_confidence):.4f}\")\n",
    "    print(f\"  Min: {blind_confidence.min():.4f}\")\n",
    "    print(f\"  Max: {blind_confidence.max():.4f}\")\n",
    "    \n",
    "    print(f\"\\nBlind ensemble agreement statistics:\")\n",
    "    print(f\"  Mean agreement: {blind_agreement.mean():.4f}\")\n",
    "    print(f\"  High agreement (>0.8): {(blind_agreement > 0.8).sum()} samples\")\n",
    "    print(f\"  Low agreement (<0.5): {(blind_agreement < 0.5).sum()} samples\")\n",
    "    \n",
    "    print(\"\\nBlind predicted class distribution:\")\n",
    "    blind_pred_counts = pd.Series(blind_labels).value_counts().sort_index()\n",
    "    for cls in blind_pred_counts.index:\n",
    "        print(f\"  Class {int(cls)}: {blind_pred_counts[cls]} samples ({100*blind_pred_counts[cls]/len(blind_labels):.2f}%)\")\n",
    "    \n",
    "    print(\"\\nClass distribution comparison (Train → Test → Blind):\")\n",
    "    for cls in sorted(class_counts.index):\n",
    "        train_pct = 100 * class_counts[cls] / len(y)\n",
    "        test_pct = 100 * pred_counts.get(cls, 0) / len(test_labels)\n",
    "        blind_pct = 100 * blind_pred_counts.get(cls, 0) / len(blind_labels)\n",
    "        print(f\"  Class {int(cls)}: {train_pct:.1f}% → {test_pct:.1f}% → {blind_pct:.1f}%\")\n",
    "    \n",
    "    # Save predictions\n",
    "    blind_output = np.column_stack([blind_pred_final, blind_labels])\n",
    "    np.savetxt('blindLabel_rf.txt', blind_output, \n",
    "               fmt='%.6f\\t%.6f\\t%.6f\\t%.6f\\t%d', \n",
    "               delimiter='\\t')\n",
    "    \n",
    "    np.savetxt('blindLabel_rf_confidence.txt', \n",
    "               np.column_stack([blind_labels, blind_confidence, blind_agreement]),\n",
    "               fmt='%d\\t%.6f\\t%.6f',\n",
    "               header='predicted_label\\tconfidence\\tensemble_agreement',\n",
    "               comments='')\n",
    "    \n",
    "    print(f\"\\n✓ Blind predictions saved to 'blindLabel_rf.txt'\")\n",
    "    print(f\"✓ Blind confidence metrics saved to 'blindLabel_rf_confidence.txt'\")\n",
    "    print(f\"  - {len(blind_labels)} predictions generated\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\n⚠️  'blindData.txt' not found - skipping blind data prediction\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️  Error processing blind data: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Final Summary\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ ALL PREDICTIONS COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFiles generated:\")\n",
    "print(f\"  1. testLabel_rf.txt ({len(test_labels)} predictions)\")\n",
    "print(f\"  2. testLabel_rf_confidence.txt (test confidence scores)\")\n",
    "try:\n",
    "    print(f\"  3. blindLabel_rf.txt ({len(blind_labels)} predictions)\")\n",
    "    print(f\"  4. blindLabel_rf_confidence.txt (blind confidence scores)\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "print(f\"  - Algorithm: Random Forest (Noise-Robust)\")\n",
    "print(f\"  - Ensemble size: {len(seeds)} models\")\n",
    "print(f\"  - Expected validation accuracy: {best_score:.4f} ± {best_std:.4f}\")\n",
    "print(f\"  - Expected macro-average AUC: {macro_auc:.4f}\")\n",
    "print(f\"  - Best configuration: Config {best_config_idx + 1}\")\n",
    "print(f\"  - Noise resistance: bagging, depth limits, min_samples constraints\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a1b14-f500-4b5e-83f0-fe09b6414c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
